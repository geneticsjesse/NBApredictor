NBApredictor workflow

1. Read in raw data files that take per game data and season summary statistics data and process using dataCleaning.py - results in a merged data frame.
2. Now that we have a merged data frame, we can filter for outliers using the outlierDetection.py script; specifically, we remove outliers that are outside of 1.5 * the interquartile range. Once we have removed those outliers, we replace any NAs with the mean for that column.
3. After removing and imputing mean values for any outliers, we check for collinearity among our predictor variables using multicollinearityCheck.py. We remove any columns that exhibit variance inflation factor values >5 and any pairwise correlations of >0.5. Specifically, this reduces the number of input features from 23 to 17.
4. Now that we have a 'cleaned' dataset, we can compute feature importance and generate learning curves using each of our specified classification models using the featureImportance.py and learning_curves.py script respectively.


5now what we have to do is create a home and away merged data frame and then merge these + feature engineer (who is home/away) 
 -- before we merge, add a new column with HOME or AWAY coded binary
 -- for loop for each year --> for loop for each team --> to check for number of wins in a row??? and if >X = 
--> re-run outlier + collinearity + importance ....



Dan meeting
- play withtest/train --> take past 4 yers and predict year 5 and different iterations of this --> not as simple as previous 7 and final year --> will want to do this a handful of times
- try to fix overfitting --> play with the depth of the trees
--- play with the model specific parameters -> see if we can improve here
- could be stacking --> will need to evalute
-- some options for stacking - can we stack all of them? but if random forest still sucks, it probably wont help the stacking 
- support vector classifier might converge at some point 
maybe we stack baives, ANN, and regression but FIRST attempt to solve overfitting --> either via bagging or fixing model specific parameters 

rf is already a bagged method so dont bag this one --> uses trees to bag, so don't use any other trees to create a bagged method ---> dan suspects it is an issue with parameter --> dont model temporal data with trees 
--- tree based models are prone to overfitting and typically not as applicable with temporal data
------ LSTM (long short term memory machines --> runs into area of deep learning)
SVM - bagging can try 
KNN - maybe can bag 

can have a table showing stacking results in the report 
use Jihao's paper as a basis for what to report (figures, tables, workflows, etc)
combine the ranking/model evaluation paramters into a single ranked table (we will we)

####### F1 score plotted - MCC tables required
DONT USE ACCURACY USE F1 SCORE and use MCC as well 

provide a picture/flowchart of the pipeline

#################################DONE #########################################
for correlation check --> can you try to run RFE on the original dataset without being cleaned by correlation based approaches  and choose target features to select to 17
--- big danger of feature by feature eleimination -> sometimes you arent looking at correlation with outcomes
--> RFE with logistic regression 17 features and see if we select the SAME 17 features as what we found from CFS --> possibly run in parallel and compare multivariate (RFE) vs univriate (CFS) and see what the results look like 
#################################DONE #########################################